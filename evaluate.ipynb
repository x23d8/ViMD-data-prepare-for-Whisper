{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb9302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for parquet files in: D:\\SPL\\Code\\PhanBietVungMien\\DAT301m\\data\\test_edited\n",
      "Folder exists: True\n",
      "Found 3 parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4bf66e39a384c23941cd2b1379348fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ViMD: 0 samples [00:00, ? samples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing at 08:19:07\n",
      "[08:49:27] Loaded 10 samples (1820.0s elapsed)\n",
      "[09:16:21] Loaded 20 samples (3433.6s elapsed)\n",
      "[09:43:09] Loaded 30 samples (5042.1s elapsed)\n",
      "[09:48:42] Processing batch 1 (32 samples)...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed\n             the \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.9.1+cu128) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\nFFmpeg version 7: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 149\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true, y_pred\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m MY_DATA.keys():\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     y_true, y_pred = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m    152\u001b[39m     wer = \u001b[32m100\u001b[39m * jiwer.wer(\n\u001b[32m    153\u001b[39m         reference=y_true,\n\u001b[32m    154\u001b[39m         hypothesis=y_pred,\n\u001b[32m    155\u001b[39m         reference_transform=JIWER_TRANS,\n\u001b[32m    156\u001b[39m         hypothesis_transform=JIWER_TRANS,\n\u001b[32m    157\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(split)\u001b[39m\n\u001b[32m    116\u001b[39m batch_count += \u001b[32m1\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Processing batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(audio_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m predictions = \u001b[43mPIPE\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE_KWARGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m y_pred.extend([out[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m predictions])\n\u001b[32m    120\u001b[39m y_true.extend(sentence_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:275\u001b[39m, in \u001b[36mAutomaticSpeechRecognitionPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Union[np.ndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m], **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    219\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[33;03m    documentation for more information.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    273\u001b[39m \u001b[33;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1448\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1445\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1446\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1447\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:271\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    268\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     processed = \u001b[38;5;28mself\u001b[39m.infer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:33\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         data.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mself\u001b[39m.ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:188\u001b[39m, in \u001b[36mPipelineChunkIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m.subiterator = \u001b[38;5;28mself\u001b[39m.infer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator), **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     processed = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start looking at the next item\u001b[39;00m\n\u001b[32m    191\u001b[39m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m.subiterator = \u001b[38;5;28mself\u001b[39m.infer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator), **\u001b[38;5;28mself\u001b[39m.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:381\u001b[39m, in \u001b[36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[39m\u001b[34m(self, inputs, chunk_length_s, stride_length_s)\u001b[39m\n\u001b[32m    378\u001b[39m         inputs = inputs.cpu().numpy()\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torchcodec_available():\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torchcodec.decoders.AudioDecoder):\n\u001b[32m    384\u001b[39m         _audio_samples = inputs.get_all_samples()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\__init__.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Note: usort wants to put Frame and FrameBatch after decoders and samplers,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# but that results in circular import.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, encoders, samplers  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\decoders\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decoder_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_cuda_backend  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\_core\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     AudioStreamMetadata,\n\u001b[32m     10\u001b[39m     ContainerMetadata,\n\u001b[32m     11\u001b[39m     get_container_metadata,\n\u001b[32m     12\u001b[39m     get_container_metadata_from_header,\n\u001b[32m     13\u001b[39m     VideoStreamMetadata,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     _add_video_stream,\n\u001b[32m     17\u001b[39m     _get_backend_details,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     seek_to_pts,\n\u001b[32m     46\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\_core\\_metadata.py:16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _get_container_json_metadata,\n\u001b[32m     18\u001b[39m     _get_stream_json_metadata,\n\u001b[32m     19\u001b[39m     create_from_file,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     23\u001b[39m SPACES = \u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStreamMetadata\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\_core\\ops.py:104\u001b[39m\n\u001b[32m    100\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m os.add_dll_directory(\u001b[38;5;28mstr\u001b[39m(ffmpeg_dir))  \u001b[38;5;66;03m# that's the actual CM\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m expose_ffmpeg_dlls():\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     ffmpeg_major_version, core_library_path = \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n\u001b[32m    109\u001b[39m create_from_file = torch._dynamo.disallow_in_graph(\n\u001b[32m    110\u001b[39m     torch.ops.torchcodec_ns.create_from_file.default\n\u001b[32m    111\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\_core\\ops.py:75\u001b[39m, in \u001b[36mload_torchcodec_shared_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     68\u001b[39m         exceptions.append((ffmpeg_major_version, e))\n\u001b[32m     70\u001b[39m traceback = (\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[32m     73\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[end of libtorchcodec loading traceback].\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     76\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m         versions 4, 5, 6, 7, and 8. On Windows, ensure you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve installed\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m         the \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfull-shared\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m version which ships DLLs.\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[33m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is not compatible with\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[33m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[33m         table:\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[33m      3. Another runtime dependency; see exceptions below.\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[33m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed\n             the \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.9.1+cu128) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\nFFmpeg version 7: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback]."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import io\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "TARGET_SR = 16000  # Whisper expects 16kHz\n",
    "\n",
    "class WhisperEvaluator:\n",
    "    def __init__(self, model_name=\"vinai/PhoWhisper-medium\", device=None):\n",
    "        \"\"\"\n",
    "        Initialize the Whisper evaluator\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model identifier for Whisper\n",
    "            device: Device to run evaluation on (cuda/cpu)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load processor and model\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load metrics\n",
    "        self.wer_metric = evaluate.load(\"wer\")\n",
    "        self.cer_metric = evaluate.load(\"cer\")\n",
    "    \n",
    "    def load_dataset(self, split=None, streaming=False, data_files=None):\n",
    "        \"\"\"\n",
    "        Load ViMD_Dataset from Hugging Face\n",
    "        \n",
    "        Args:\n",
    "            split: Dataset split to load\n",
    "            streaming: Whether to use streaming mode\n",
    "            data_files: Specific parquet files to download\n",
    "        \"\"\"\n",
    "        if data_files:\n",
    "            print(f\"Loading ViMD_Dataset from specific files: {data_files}\")\n",
    "            dataset = load_dataset(\n",
    "                \"nguyendv02/ViMD_Dataset\",\n",
    "                data_files=data_files,\n",
    "                split=\"train\",\n",
    "                streaming=streaming\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Loading ViMD_Dataset (split: {split})...\")\n",
    "            dataset = load_dataset(\n",
    "                \"nguyendv02/ViMD_Dataset\",\n",
    "                split=split,\n",
    "                streaming=streaming\n",
    "            )\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "        return dataset\n",
    "    \n",
    "    def prepare_dataset(self, batch):\n",
    "        \"\"\"\n",
    "        Prepare audio data from bytes without using torchcodec\n",
    "        \n",
    "        Args:\n",
    "            batch: Single sample from dataset\n",
    "            \n",
    "        Returns:\n",
    "            Batch with prepared input_features and labels\n",
    "        \"\"\"\n",
    "        # Read audio from bytes using soundfile\n",
    "        audio_bytes = batch[\"audio\"][\"bytes\"]\n",
    "        with io.BytesIO(audio_bytes) as f:\n",
    "            array, sr = sf.read(f, dtype=\"float32\")\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        array = torch.from_numpy(array)\n",
    "        \n",
    "        # Convert stereo to mono if needed\n",
    "        if array.ndim > 1:\n",
    "            array = array.mean(dim=1)\n",
    "        \n",
    "        # Resample to target sampling rate if needed\n",
    "        if sr != TARGET_SR:\n",
    "            array = torchaudio.functional.resample(array, sr, TARGET_SR)\n",
    "        \n",
    "        # Extract features using Whisper processor\n",
    "        batch[\"input_features\"] = self.processor.feature_extractor(\n",
    "            array, \n",
    "            sampling_rate=TARGET_SR\n",
    "        ).input_features[0]\n",
    "        \n",
    "        # Prepare labels (transcription)\n",
    "        # Check different possible field names\n",
    "        text = batch.get(\"text\") or batch.get(\"transcription\") or batch.get(\"sentence\", \"\")\n",
    "        batch[\"labels\"] = self.processor.tokenizer(\n",
    "            text, \n",
    "            max_length=448, \n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def transcribe(self, input_features):\n",
    "        \"\"\"\n",
    "        Transcribe audio using Whisper\n",
    "        \n",
    "        Args:\n",
    "            input_features: Preprocessed audio features\n",
    "            \n",
    "        Returns:\n",
    "            Transcription text\n",
    "        \"\"\"\n",
    "        # Ensure input_features is a tensor on the correct device\n",
    "        if not isinstance(input_features, torch.Tensor):\n",
    "            input_features = torch.tensor(input_features)\n",
    "        \n",
    "        if input_features.ndim == 2:\n",
    "            input_features = input_features.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        input_features = input_features.to(self.device)\n",
    "        \n",
    "        # Generate transcription\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = self.model.generate(\n",
    "                input_features,\n",
    "                language=\"vi\",\n",
    "                task=\"transcribe\"\n",
    "            )\n",
    "        \n",
    "        # Decode transcription\n",
    "        transcription = self.processor.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        return transcription\n",
    "    \n",
    "    def evaluate_dataset(self, dataset=None, split=\"test\", max_samples=None, data_files=None):\n",
    "        \"\"\"\n",
    "        Evaluate Whisper on the dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset: Pre-loaded dataset (optional)\n",
    "            split: Split to evaluate if dataset not provided\n",
    "            max_samples: Maximum number of samples to evaluate\n",
    "            data_files: Specific data files to load\n",
    "        \"\"\"\n",
    "        if dataset is None:\n",
    "            dataset = self.load_dataset(split=split, data_files=data_files)\n",
    "        \n",
    "        predictions = []\n",
    "        references = []\n",
    "        results = []\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if max_samples:\n",
    "            if hasattr(dataset, 'select'):\n",
    "                dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "        \n",
    "        print(f\"\\nEvaluating on {len(dataset) if hasattr(dataset, '__len__') else 'streaming'} samples...\")\n",
    "        \n",
    "        for idx, sample in enumerate(tqdm(dataset)):\n",
    "            # Stop if max_samples reached (for streaming datasets)\n",
    "            if max_samples and idx >= max_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Prepare the sample (extract features from audio bytes)\n",
    "                prepared = self.prepare_dataset(sample)\n",
    "                \n",
    "                # Get input features\n",
    "                input_features = prepared[\"input_features\"]\n",
    "                \n",
    "                # Get reference transcription\n",
    "                reference = sample.get(\"text\") or sample.get(\"transcription\") or sample.get(\"sentence\", \"\")\n",
    "                \n",
    "                # Transcribe\n",
    "                prediction = self.transcribe(input_features)\n",
    "                \n",
    "                predictions.append(prediction)\n",
    "                references.append(reference)\n",
    "                \n",
    "                # Store individual result\n",
    "                results.append({\n",
    "                    \"index\": idx,\n",
    "                    \"prediction\": prediction,\n",
    "                    \"reference\": reference\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing sample {idx}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(\"\\nCalculating metrics...\")\n",
    "        wer = self.wer_metric.compute(predictions=predictions, references=references)\n",
    "        cer = self.cer_metric.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        avg_pred_length = np.mean([len(p) for p in predictions])\n",
    "        avg_ref_length = np.mean([len(r) for r in references])\n",
    "        \n",
    "        metrics = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"dataset\": \"ViMD_Dataset\",\n",
    "            \"split\": split if split else \"custom\",\n",
    "            \"num_samples\": len(predictions),\n",
    "            \"wer\": wer * 100,  # Convert to percentage\n",
    "            \"cer\": cer * 100,  # Convert to percentage\n",
    "            \"avg_prediction_length\": avg_pred_length,\n",
    "            \"avg_reference_length\": avg_ref_length,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return metrics, results\n",
    "    \n",
    "    def print_results(self, metrics, sample_results=None, num_examples=5):\n",
    "        \"\"\"\n",
    "        Print evaluation results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: {metrics['model_name']}\")\n",
    "        print(f\"Dataset: {metrics['dataset']} ({metrics['split']} split)\")\n",
    "        print(f\"Number of samples: {metrics['num_samples']}\")\n",
    "        print(f\"\\nWord Error Rate (WER): {metrics['wer']:.2f}%\")\n",
    "        print(f\"Character Error Rate (CER): {metrics['cer']:.2f}%\")\n",
    "        print(f\"\\nAverage prediction length: {metrics['avg_prediction_length']:.1f} characters\")\n",
    "        print(f\"Average reference length: {metrics['avg_reference_length']:.1f} characters\")\n",
    "        \n",
    "        if sample_results and num_examples > 0:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"SAMPLE PREDICTIONS (first {num_examples})\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            for i, result in enumerate(sample_results[:num_examples]):\n",
    "                print(f\"\\n--- Sample {result['index'] + 1} ---\")\n",
    "                print(f\"Reference:  {result['reference']}\")\n",
    "                print(f\"Prediction: {result['prediction']}\")\n",
    "    \n",
    "    def save_results(self, metrics, results, output_file=\"phowhisper_medium_evaluation_results.json\"):\n",
    "        \"\"\"\n",
    "        Save evaluation results to JSON file\n",
    "        \"\"\"\n",
    "        output_data = {\n",
    "            \"metrics\": metrics,\n",
    "            \"detailed_results\": results\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main evaluation function\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    MODEL_NAME = \"vinai/PhoWhisper-medium\"\n",
    "    MAX_SAMPLES = 100  # Set to None for all samples\n",
    "    OUTPUT_FILE = \"PhoWhisperMed_vimd_evaluation.json\"\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = WhisperEvaluator(model_name=MODEL_NAME)\n",
    "    \n",
    "    # Load only test files or anything you want\n",
    "    metrics, results = evaluator.evaluate_dataset(\n",
    "        data_files=\"data/valid-*.parquet\",\n",
    "        max_samples=MAX_SAMPLES\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    evaluator.print_results(metrics, results, num_examples=5)\n",
    "    \n",
    "    # Save results\n",
    "    evaluator.save_results(metrics, results, output_file=OUTPUT_FILE)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
